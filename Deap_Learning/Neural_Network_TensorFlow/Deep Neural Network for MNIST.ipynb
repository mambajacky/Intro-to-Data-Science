{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification \n",
    "\n",
    "This problem is referred to as the \"Hello World\" of deep learning. The dataset is called MNIST and refers to handwritten digit recognition.\n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (from 0 to 9), this is a classification problem with 10 classes.\n",
    "\n",
    " We'll build a neural network of 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# tensorflow-datasets provides lots of datasets ready for modeling\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "1. First time the load function will download the dataset\n",
    "2. Subsequent calls will reuse the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_info=True, it will return a tuple containing the dataset and info associated with the dataset\n",
    "# as_supervised=True, the returned dataset will contain (input, target)\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       " 'train': <PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    version=3.0.1,\n",
       "    description='The MNIST database of handwritten digits.',\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=70000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 60000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset to training and test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "\n",
    "# use 10% of training data as validation data\n",
    "# tf.cast can casts a tensor to a new type\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "# dataset.map function applies a custom function to a given dataset\n",
    "scaled_train_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the training data and split validation data from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer size defines number of samples for each shuffle\n",
    "buffer_size = 10000\n",
    "\n",
    "shuffled_train_validation_data = scaled_train_validation_data.shuffle(buffer_size)\n",
    "\n",
    "# dataset.take(N) function extracts first N samples from the dataset\n",
    "# dataset.skip(N) function extracts all samples except first N samples from the dataset\n",
    "validation_data = shuffled_train_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching \n",
    "\n",
    "1. Batching is used in SGD where in each epoch, updating the weights as many times as number of batches\n",
    "2. Validation data is not required for batching as it's only used in forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size defines how many samples a model should take in each batch\n",
    "batch_size = 100\n",
    "\n",
    "# here we also batch validation data and test data because model expects them in batch form too\n",
    "train_data = train_data.batch(batch_size)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inputs and targets for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split validation data to inputs and targets as they are needed in fitting the model\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hypeparameters\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            # tf.keras.layers.Flatten(original shape) transforms a tensor into a vector\n",
    "                            # input layer\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            # tf.keras.layers.Dense(output size) takes inputs and calculates (xw + b)\n",
    "                            # and can also apply activation function\n",
    "                            # hiddent layers\n",
    "                            tf.keras.layers.Dense(units=hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(units=hidden_layer_size, activation='relu'),\n",
    "                            # output layer\n",
    "                            tf.keras.layers.Dense(units=output_size, activation='softmax')\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose objection function and optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam combines momentum and AdaGrad\n",
    "# sparse_categorical_crossentropy applies one-hot encoding to the targets so target becomes a vector instead of scalar\n",
    "# we can add metric that we want to compute throughout the training\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 7s - loss: 0.4250 - accuracy: 0.8794 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "540/540 - 5s - loss: 0.1744 - accuracy: 0.9490 - val_loss: 0.1548 - val_accuracy: 0.9572\n",
      "Epoch 3/10\n",
      "540/540 - 6s - loss: 0.1343 - accuracy: 0.9600 - val_loss: 0.1232 - val_accuracy: 0.9643\n",
      "Epoch 4/10\n",
      "540/540 - 6s - loss: 0.1071 - accuracy: 0.9686 - val_loss: 0.1095 - val_accuracy: 0.9688\n",
      "Epoch 5/10\n",
      "540/540 - 6s - loss: 0.0919 - accuracy: 0.9728 - val_loss: 0.0970 - val_accuracy: 0.9717\n",
      "Epoch 6/10\n",
      "540/540 - 6s - loss: 0.0801 - accuracy: 0.9754 - val_loss: 0.0862 - val_accuracy: 0.9763\n",
      "Epoch 7/10\n",
      "540/540 - 6s - loss: 0.0711 - accuracy: 0.9787 - val_loss: 0.0877 - val_accuracy: 0.9740\n",
      "Epoch 8/10\n",
      "540/540 - 6s - loss: 0.0635 - accuracy: 0.9809 - val_loss: 0.0659 - val_accuracy: 0.9798\n",
      "Epoch 9/10\n",
      "540/540 - 6s - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.0652 - val_accuracy: 0.9808\n",
      "Epoch 10/10\n",
      "540/540 - 5s - loss: 0.0506 - accuracy: 0.9840 - val_loss: 0.0623 - val_accuracy: 0.9807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15b4d19bcc8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# we can feed a 2-tuple object or two arrays, train_data is a tuple containing both inputs and targets\n",
    "model.fit(train_data, epochs=num_epochs, validation_data=(validation_inputs,validation_targets),validation_steps=1,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key information:\n",
    "1. 540 is number of batches per epoch\n",
    "2. Accuracy measures % of outputs matching with targets\n",
    "3. Train accuracy is the average accuracy of all batches in an epoch, validation accuracy is the true accuracy of whole validation datasets for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.1014 - accuracy: 0.9708\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.10. Test accuracy: 97.08%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python TF",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
